{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we write an implementation of the awesome paper Few-Shot Learning Through an Information Retrieval Lens by [Eleni Triantafillou](http://www.cs.toronto.edu/~eleni/), [Richard Zemel](http://www.cs.toronto.edu/~zemel/inquiry/home.php), and [http://www.cs.toronto.edu/~urtasun/]. \n",
    "\n",
    "First of all, the implementation associated to the paper is already public in [Eleni's github repo](https://github.com/eleniTriantafillou/few_shot_mAP_public). It is clearly written and the goal here would be to break it down into different parts and use PyTorch instead of Tensorflow. I will also comment a little bit about the reasons (as I understand them) for some of their decisions. \n",
    "\n",
    "I welcome feedback and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot learning\n",
    "\n",
    "The goal of the paper is to propose a new model to tackle the problem of learning from few samples. The main idea as described in the paper is to \"...extract as much information as possible from each training batch.\" \n",
    "\n",
    "In order to accomplish the goal each data point will be understood as a query that ranks the remaining ones basted on its predicted relevance. \n",
    "\n",
    "To make this more precise we a definitions.\n",
    "\n",
    "---\n",
    "\n",
    "**Mean Average Precision (mAP):** We start with the data $\\chi=\\{x_1,\\ldots,x_n\\}$ of points with associated classes $c_1,\\ldots,c_n$. Let $\\mathcal{B} \\subset \\chi $ be a batch. The set Rel$^{x_i}$ consist of all the elements on the batch that have the same class as $x_i$ and after ranking them with respect to how relevant they are to $x_i$ we obtain an ordered list $O^{x_i}$ (with smallest indexes been more relevant), we compute\n",
    "\n",
    "$$AP^{x_i}= \\sum_j \\frac{\\left|\\{k\\leq j \\colon O^{x_i}[k]\\in Rel^{x_i}\\}\\right|}{j*\\left|Rel^{x_i}\\right|}$$  \n",
    "\n",
    "where the sum runs over those $j$ such that $x_j \\in \\mathcal{B}$ and $O^{x_i}[j] \\in Rel^{x_i}$. That is, $AP^{x_i}$ is just the average among the precisions on the ranking associated to $x_i$. In particular, this attains the largest value if the ranking is perfect, in which case we would have $AP^{x_i}=1$.\n",
    "\n",
    "When we run this over all the elements of the batch we obtain the mean Average Precision, that is \n",
    "\n",
    "$$mAP = \\frac{1}{|\\mathcal{B}|} \\sum_{x_i \\in \\mathcal{B}}AP^{x_i}.$$\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "Our goal then is to maximize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structural Support Vector Machines (SSVM):** This measure is slightly more involved. We need an scoring function $F(x,y;w)$ that depends on the inputs x, the outputs y, and some parameters/weights $w$. We also need a loss $L(y_{GT},\\hat{y})$ which compares the ground truth values $y_{GT}$ with the predicted values $\\hat{y}$. Then the margin-rescaled SSVM has as goal to find the weights that minimize the expresion \n",
    "\n",
    "$$\\mathbb{E}[\\max_{\\hat{y}} \\{ L(y_{GT},\\hat{y}) - F(x,y_{GT};w) +F(x,\\hat{y};w)\\}]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Direct Loss Minimization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data \n",
    "\n",
    "We use the omniglot dataset which can be obtained [here](https://github.com/brendenlake/omniglot) [1]. It consists on ???? \n",
    "for our purposes we have located the data contained in the python folder of the omniglot repository inside a folder called omniglot.\n",
    "\n",
    "A huge part of the process will be to deal with the preprocessing of the dataset. We have the following steps:\n",
    "\n",
    "- Indexing.\n",
    "- Rotating.\n",
    "- Selecting.\n",
    "- separating.\n",
    "???\n",
    "\n",
    "Let's start this process by creating a constant to hold the path to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OMNIGLOT_FOlDER=\"./data/omniglot/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use the system utilities via the OS module to create a dictionary mapping the index of each training image to its location on the disk. We do this for the train dataset, the validation dataset and the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def load_dataset():\n",
    "    \n",
    "    #Initialize the dicts\n",
    "    \n",
    "    images_train_dic={}\n",
    "    labels_train=[]\n",
    "    \n",
    "    images_val_dic={}\n",
    "    labels_val=[]\n",
    "    \n",
    "    images_test_dic={}\n",
    "    labels_test=[]\n",
    "    \n",
    "    #Get the list of classes for train\n",
    "    with open(\"./data/dataset_splits/omniglot/train.txt\") as f:\n",
    "        train_classes = [line.strip() for line in f]\n",
    "    \n",
    "    #Get the list of classes for val\n",
    "    with open(\"./data/dataset_splits/omniglot/val.txt\") as f:\n",
    "        val_classes = [line.strip() for line in f]\n",
    "        \n",
    "    #Get the list of classes for test\n",
    "    with open(\"./data/dataset_splits/omniglot/test.txt\") as f:\n",
    "        test_classes = [line.strip() for line in f]\n",
    "            \n",
    "    \n",
    "    # We will go over all the images. The classes are given by \n",
    "    # the name of the alphabet and folder the image is contain in.\n",
    "    \n",
    "    #Initialized the indexes\n",
    "    index_train = 0\n",
    "    index_val = 0\n",
    "    index_test =0\n",
    "    num_classes = -1\n",
    "    \n",
    "    for c in train_classes+val_classes+test_classes:\n",
    "        \n",
    "        num_classes +=1\n",
    "        \n",
    "        # c looks like alphabetName/character\n",
    "        # so we get the alphabet out of it.\n",
    "        alphabet,char = c.split(\"/\")\n",
    "        \n",
    "        # As there are two folders we need to decide\n",
    "        # to which folder the alphabet belongs to.\n",
    "        \n",
    "        if os.path.isdir(\"./data/omniglot/images_background/\"+alphabet):\n",
    "            alphabet_path = \"./data/omniglot/images_background/\"+alphabet\n",
    "        \n",
    "        elif os.path.isdir(\"./data/omniglot/images_evaluation/\"+alphabet):\n",
    "            alphabet_path = \"./data/omniglot/images_evaluation/\"+alphabet\n",
    "        \n",
    "        else:\n",
    "            print(\"No such alphabet\")\n",
    "        \n",
    "        # Next we collect the images in the folder\n",
    "        \n",
    "        for img in os.listdir(alphabet_path+\"/\"+char):\n",
    "            image_location = alphabet_path+\"/\"+char+\"/\"+img\n",
    "            \n",
    "            if c in train_classes:\n",
    "                images_train_dic[index_train]=image_location\n",
    "                index_train+=1\n",
    "                labels_train.append(c)\n",
    "            \n",
    "            elif c in val_classes:\n",
    "                images_val_dic[index_val]=image_location\n",
    "                index_val+=1\n",
    "                labels_val.append(c)\n",
    "                \n",
    "            elif c in test_classes:\n",
    "                images_test_dic[index_test]=image_location\n",
    "                index_test+=1\n",
    "                labels_test.append(c)\n",
    "            else:\n",
    "                print(\"class not found\")\n",
    "        \n",
    "    #We collect some informative data.    \n",
    "    print(\"There are %d classes\"%num_classes)\n",
    "    print(\"There are %d training images in %d classes\"%\\\n",
    "          (len(images_train_dic),len(set(labels_train))))\n",
    "    print(\"There are %d validation images in %d classes\"%\\\n",
    "          (len(images_val_dic),len(set(labels_val))))\n",
    "    print(\"There are %d test images in %d classes\"%\\\n",
    "          (len(images_test_dic),len(set(labels_test))))\n",
    "    \n",
    "    return images_train_dic, np.array(labels_train), images_val_dic, \\\n",
    "        np.array(labels_val),images_test_dic,np.array(labels_test)         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we obtained the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1622 classes\n",
      "There are 14400 training images in 720 classes\n",
      "There are 9600 validation images in 480 classes\n",
      "There are 8460 test images in 423 classes\n"
     ]
    }
   ],
   "source": [
    "images_train_dic, labels_train, images_val_dic,\\\n",
    "labels_val,images_test_dic,labels_test=load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to the data, we should learn how to create the batches. But we need to be careful when doing this, since we depend on two parameters, we want to be able to choose the choose a batch_size, but we also want to assure that the points chosen belong to a certain number of classes. We also need to know for which of the datasets (train, val, test) we are getting the data from, we use the split variable to indicate this. The following method will allow us to do just that. \n",
    "\n",
    "Note that a batch consists on images associated to a list of random characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Libraries needed for image handling\n",
    "from scipy.misc import imread,imresize\n",
    "\n",
    "#Constant size (of images) to be used\n",
    "HEIGHT=28\n",
    "WIDTH=28\n",
    "\n",
    "\n",
    "def get_batch(batch_size,num_classes,split):\n",
    "    \n",
    "    #What split are we getting the bacth from \n",
    "    if split=='train':\n",
    "        classes=labels_train\n",
    "    elif split=='val':\n",
    "        classes=labels_val\n",
    "    elif split=='test':\n",
    "        classes=labels_test\n",
    "    else:\n",
    "        print(\"There is not such dataset\")\n",
    "    \n",
    "    # Randomly pick num_classes\n",
    "    selected_indexes = np.random.choice(len(classes),num_classes)\n",
    "    selected_classes = classes[selected_indexes]\n",
    "    \n",
    "    # Get the indexes of all images within the selected classes\n",
    "    if split=='train':\n",
    "        indexes_images=[]\n",
    "        for a_class in selected_classes:\n",
    "            indexes_images += list(np.where(labels_train==a_class)[0])\n",
    "    elif split=='val':\n",
    "        indexes_images=[]\n",
    "        for a_class in selected_classes:\n",
    "            indexes_images += list(np.where(labels_val==a_class)[0])\n",
    "    if split=='test':\n",
    "        indexes_images=[]\n",
    "        for a_class in selected_classes:\n",
    "            indexes_images += list(np.where(labels_test==a_class)[0])\n",
    "    \n",
    "    #Randomnly select batch_size of them\n",
    "    selected_indexes = np.random.choice(indexes_images,batch_size,replace=False)\n",
    "    \n",
    "    \n",
    "    #Get the images and resize them\n",
    "    batch_imgs = np.array([])\n",
    "    if split =='train':\n",
    "        for i in selected_indexes:\n",
    "            img_path=images_train_dic[i]\n",
    "            img_array=imread(img_path,mode='L')\n",
    "            img_array=imresize(img_array,(HEIGHT,WIDTH,1),interp='bicubic')\n",
    "            #Reshape and transpose so it looks like [1,n_channels,height,width]\n",
    "            img_array=img_array.reshape((1,HEIGHT,WIDTH,1)).transpose(0,3,1,2)\n",
    "            if batch_imgs.shape[0]==0: #First image\n",
    "                batch_imgs=img_array\n",
    "            else:\n",
    "                batch_imgs=np.concatenate((batch_imgs,img_array),axis=0)\n",
    "    \n",
    "        #We collect the labels\n",
    "        selected_labels=labels_train[selected_indexes]\n",
    "    \n",
    "    elif split =='val':\n",
    "        for i in selected_indexes:\n",
    "            img_path=images_val_dic[i]\n",
    "            img_array=imread(img_path,mode='L')\n",
    "            img_array=imresize(img_array,(HEIGHT,WIDTH,1),interp='bicubic')\n",
    "            #Reshape and transpose so it looks like [1,n_channels,height,width]\n",
    "            img_array=img_array.reshape((1,HEIGHT,WIDTH,1)).transpose(0,3,1,2)\n",
    "            if batch_imgs.shape[0]==0: #First image\n",
    "                batch_imgs=img_array\n",
    "            else:\n",
    "                batch_imgs=np.concatenate((batch_imgs,img_array),axis=0)\n",
    "    \n",
    "        #We collect the labels\n",
    "        selected_labels=labels_val[selected_indexes]\n",
    "    \n",
    "    if split =='test':\n",
    "        for i in selected_indexes:\n",
    "            img_path=images_test_dic[i]\n",
    "            img_array=imread(img_path,mode='L')\n",
    "            img_array=imresize(img_array,(HEIGHT,WIDTH,1),interp='bicubic')\n",
    "            #Reshape and transpose so it looks like [1,n_channels,height,width]\n",
    "            img_array=img_array.reshape((1,HEIGHT,WIDTH,1)).transpose(0,3,1,2)\n",
    "            if batch_imgs.shape[0]==0: #First image\n",
    "                batch_imgs=img_array\n",
    "            else:\n",
    "                batch_imgs=np.concatenate((batch_imgs,img_array),axis=0)\n",
    "    \n",
    "        #We collect the labels\n",
    "        selected_labels=labels_test[selected_indexes]\n",
    "    \n",
    "    return batch_imgs,selected_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can get batches of points to train we go to the fun part: Building the model.\n",
    "\n",
    "# The models\n",
    "\n",
    "\n",
    "\n",
    "As we talk before the model would be...\n",
    "\n",
    "\n",
    "There are two models we need to consider, the one associated with Direct Loss Minimization (DLM) and the one associated with ??? (SSVM), we build these models inheriting properties of a larger generic model. \n",
    "\n",
    "As the number of paramerters in each model is quite large, we keep them in a config object, as above we have a generic config object and let a simple inheritance handle each of the cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        #Learning rate\n",
    "        self.lr = 0.001\n",
    "        \n",
    "#         #Schedule\n",
    "#         self.ada_learning_rate = False\n",
    "#         self.start_decr_lr = 2000\n",
    "#         self.mult_lr_value = 0.5\n",
    "#         self.freq_decr_lr = 2000\n",
    "#         self.smallest_lr = 0.0001\n",
    "        \n",
    "        #Optimization \n",
    "        self.optimizer = \"ADAM\"\n",
    "        self.epsilon = 1\n",
    "        self.alpha = 10\n",
    "        \n",
    "        #Batch \n",
    "        self.batch_size= 16 #128\n",
    "        self.nway = 4# 16 #Number of classes in each batch\n",
    "        \n",
    "\n",
    "class ConfigDLM(Config):\n",
    "    \n",
    "    def __init__(self):  \n",
    "        Config.__init__(self)\n",
    "        self.optimization_framework=\"DLM\"\n",
    "        self.positive_update=True\n",
    "        \n",
    "class ConfigSSVM(Config):\n",
    "    \n",
    "    def __init__(self):  \n",
    "        Config.__init__(self)\n",
    "        self.optimization_framework=\"SSVM\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And before the models, let's import our Deep Learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is configured as follows:\n",
    "\n",
    "## The embedding part\n",
    "\n",
    "We start with 4 Layers, each layer consists of a convolution with 64 filters (read out channels), follow by a normalization and a relu activation, to finish the layer we have a max pooling. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        \n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "        self.config=config\n",
    "        self.losses = [] # A list to keep our losses\n",
    "        \n",
    "        #Layers for embedding\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=64,\n",
    "                kernel_size=[3,3]\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d([2,2],padding=1)\n",
    "        )\n",
    "        \n",
    "        self.layer2= torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=[3,3],\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d([2,2],padding=1)\n",
    "        )\n",
    "        \n",
    "        self.layer3= torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=[3,3],\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d([2,2],padding=1)\n",
    "        )\n",
    "        \n",
    "        self.layer4= torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=[3,3],\n",
    "            ),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d([2,2],padding=1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we started with tensors of shape n x 1 x 28 x 28, where n is the number of samples in the batch, we obtain a tensor of size n x 64 x 1 x 1 . We reshape this tensor to get an embedding of size 64 for each image, that is we obtain a tensor of size n x 64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(self,x): #Embedding\n",
    "        \n",
    "    # Embedding part 1\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "        \n",
    "    # Embedding part 2\n",
    "    embedding = x.view(x.shape[0],-1)\n",
    "\n",
    "    return  embedding\n",
    "\n",
    "# Add method to the class\n",
    "Model.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The positive and negative split:\n",
    "\n",
    "Let $\\mathcal{B}=\\{x_1,\\ldots,x_n\\}$ be a batch. If $x_i \\in \\mathcal{B}$ let $c_i$ be the class they belong to. For each $x_i$ We separate the points into the ones in the same class $\\mathcal{P}^{c_i}$ (positive) and the ones in other classes $\\mathcal{N}^{c_i}$ (negative) so that $\\mathcal{B} = \\mathcal{P}^{c_i}\\cup \\mathcal{N}^{c_i}$.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_positive_negative_splits(self,batch):\n",
    "        \n",
    "    pos_positions, neg_positions = [],[] #list of lists\n",
    "    pos_sizes, neg_sizes = [], [] # sizes of each pos, neg split\n",
    "\n",
    "    batch_labels = batch[1] \n",
    "    batch_size = len(batch_labels)\n",
    "    \n",
    "    def separate_for_current(current):\n",
    "\n",
    "        current_pos, current_neg =[],[]\n",
    "        current_label = batch_labels[current]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if batch_labels[i]==current_label:\n",
    "                current_pos.append(i)\n",
    "            else:\n",
    "                current_neg.append(i)\n",
    "\n",
    "        return current_pos, current_neg\n",
    "\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        current_pos, current_neg = separate_for_current(i)\n",
    "        pos_positions.append(current_pos)\n",
    "        neg_positions.append(current_neg)\n",
    "\n",
    "        pos_sizes.append(len(current_pos))\n",
    "        neg_sizes.append(len(current_neg))\n",
    "\n",
    "\n",
    "    max_positive_length, max_negative_length = \\\n",
    "        max(pos_sizes), max(neg_sizes)\n",
    "\n",
    "    # We pad with zeros so each list has same size\n",
    "    for i in range(batch_size):\n",
    "        pos_positions[i]+=[0]*(max_positive_length-len(pos_positions[i]))\n",
    "        neg_positions[i]+=[0]*(max_negative_length-len(neg_positions[i]))\n",
    "    \n",
    "    return np.array(pos_positions), np.array(neg_positions),\\\n",
    "            np.array(pos_sizes), np.array(neg_sizes)\n",
    "\n",
    "    \n",
    "# Add method to the class\n",
    "Model.get_positive_negative_splits = get_positive_negative_splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The inference part:\n",
    "This is the main computation and needs to be split into different parts:\n",
    "\n",
    "A wraper method (inference) that takes a batch and transforms the data into tensors which are fed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(self,batch):\n",
    "\n",
    "    pos_positions, neg_positions, pos_sizes, \\\n",
    "         neg_sizes = self.get_positive_negative_splits(batch)\n",
    "\n",
    "    batch_X_tensor = torch.from_numpy(batch[0]).float()\n",
    "    batch_X_tensor = Variable(batch_X_tensor,\n",
    "                                             requires_grad=False)\n",
    "\n",
    "\n",
    "    #Send the tensor to computation\n",
    "    mAP_score_std,mAP_score_aug,mAP_score_GT = self.inference_mAP(\n",
    "        batch_X_tensor,\n",
    "        pos_positions,\n",
    "        neg_positions,\n",
    "        pos_sizes,\n",
    "        neg_sizes\n",
    "    )\n",
    "    \n",
    "    return mAP_score_std,mAP_score_aug,mAP_score_GT\n",
    "# Add method to the class\n",
    "Model.inference = inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method to compute the mAP loss (inference_mAP), this needs to run over each of the samples and do a computation over each of them.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_mAP(self,batch_X,pos_positions,\n",
    "    neg_positions,pos_sizes,neg_sizes):\n",
    "        \n",
    "    batch_size = batch_X.shape[0]\n",
    "\n",
    "    # Embed the batch\n",
    "    emb_X=self.forward(batch_X)\n",
    "\n",
    "    #As we are computing cosine similarity, we normalize the vectors\n",
    "    norms = torch.norm(emb_X,p=2,dim=1)  #.detach()\n",
    "    emb_X = emb_X.div(norms.view([-1,1]))\n",
    "\n",
    " \n",
    "    mAP_score_std=Variable(torch.zeros(1))\n",
    "    mAP_score_aug=Variable(torch.zeros(1))\n",
    "    mAP_score_GT=Variable(torch.zeros(1))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        current = emb_X[i]\n",
    "        \n",
    "        current_num_positive = int(pos_sizes[i])\n",
    "        current_num_negative = int(neg_sizes[i])\n",
    "        \n",
    "        # We need at least a negative sample\n",
    "        if current_num_negative==0:\n",
    "            continue        \n",
    "\n",
    "    \n",
    "        positive_for_current = \\\n",
    "            emb_X[pos_positions[i][:current_num_positive],:]\n",
    "            \n",
    "        negative_for_current = \\\n",
    "            emb_X[neg_positions[i][:current_num_negative],:]\n",
    "        \n",
    "        phi_pos,phi_neg,AP_score_std,AP_score_aug,AP_score_GT =\\\n",
    "            self.inference_individual(current,\n",
    "                         positive_for_current,negative_for_current,\n",
    "                         current_num_positive,current_num_negative)\n",
    "        \n",
    "        mAP_score_std+=AP_score_std\n",
    "        mAP_score_aug+=AP_score_aug\n",
    "        mAP_score_GT+=AP_score_GT\n",
    "    \n",
    "    #This is not in the paper, but is in their implementation\n",
    "    #it makes sense to normalize so I include it as well.\n",
    "    \n",
    "    mAP_score_std/=float(batch_size)\n",
    "    mAP_score_aug/=float(batch_size)\n",
    "    mAP_score_GT/=float(batch_size)\n",
    "\n",
    "    return mAP_score_std,mAP_score_aug,mAP_score_GT\n",
    "\n",
    "# Add method to the class\n",
    "Model.inference_mAP = inference_mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method for doing inference for each data point (inference_individual) in the batch. If the data point is $x_i$, it takes the positive and negative splits and compute the different average precision (AP). Note that for the computation of the augmented one, we followed the same technique as the author and use a dynamic algorithm found in [2], we use the class created by Eleni found [here](https://github.com/eleniTriantafillou/few_shot_mAP_public/blob/master/src/utils/loss_aug_AP.py), we put the code in the utils folder. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.loss_aug_AP import LossAugmentedInferenceAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_individual(self,current,\n",
    "                         positive_for_current,negative_for_current,\n",
    "                         current_num_positive,current_num_negative):\n",
    "    \n",
    "    # Compute similarities\n",
    "    S_pos = torch.matmul(current,positive_for_current.t())\n",
    "    S_neg = torch.matmul(current,negative_for_current.t())\n",
    "\n",
    "    # sorts the values of the similarities\n",
    "    phi_pos, _ = S_pos.sort()\n",
    "    phi_neg, _ = S_neg.sort()\n",
    "\n",
    "\n",
    "    #Need to compute three values\n",
    "\n",
    "    # 1. Score of standard inference F_std \n",
    "\n",
    "    #Make them into 2-tensors\n",
    "    phi_pos=phi_pos.view(-1,1) # \"col vector\"\n",
    "    phi_neg=phi_neg.view(1,-1) # \"row vector\"\n",
    "    \n",
    "    # repeat the row vector vertically \n",
    "    # size = current number positive X current number negative\n",
    "    phi_pos_repeated = phi_pos.repeat(1,current_num_negative) \n",
    "\n",
    "    # size = current number positive X current number negative\n",
    "    phi_neg_repeated = phi_neg.repeat(current_num_positive,1)\n",
    "    \n",
    "    # The following matrix encodes which one is greater\n",
    "    Y_ij= (phi_pos_repeated > phi_neg_repeated).float()\n",
    "\n",
    "    #Scale to -1/+1\n",
    "    Y_ij = 2*Y_ij-1\n",
    "\n",
    "    # Finally compute the Score of standard inference\n",
    "    F_std = Y_ij*(phi_pos_repeated-phi_neg_repeated)\n",
    "    AP_score_std = F_std.sum()/float(current_num_positive*current_num_negative)\n",
    "\n",
    "\n",
    "    #2. Score of the Loss-augmeneted inferred ranking\n",
    "    #print(\"LOSS-AUGMENTED NEEDS TO BE DONE\")\n",
    "    \n",
    "    loss_augmented = LossAugmentedInferenceAP(\n",
    "        phi_pos.data.numpy()[0],phi_neg.data.numpy()[0],\n",
    "        self.config.epsilon,\n",
    "        #self.config.positive_update,\n",
    "    )\n",
    "    Y_aug_ij = loss_augmented.ranking\n",
    "    #Y_aug_ij = torch.zeros_like(Y_ij)#PLACEHOLDER\n",
    "    F_aug =Y_aug_ij*(phi_pos_repeated-phi_neg_repeated)\n",
    "    AP_score_aug = F_aug.sum()/float(current_num_positive*current_num_negative)\n",
    "\n",
    "\n",
    "\n",
    "    #3. Score of the groundtruth\n",
    "    Y_GT_ij = torch.ones_like(Y_ij)\n",
    "    F_GT =Y_GT_ij*(phi_pos_repeated-phi_neg_repeated)\n",
    "    AP_score_GT = F_GT.sum()/float(current_num_positive*current_num_negative)\n",
    "        \n",
    "    return phi_pos,phi_neg,AP_score_std,AP_score_aug,AP_score_GT\n",
    "\n",
    "\n",
    "# Add method to the class\n",
    "Model.inference_individual = inference_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to compute the different values we can compute the loss according to the different frameworks that we are considering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(self,mAP_score_std,mAP_score_aug,mAP_score_GT):\n",
    "    \n",
    "    if self.config.optimization_framework == \"SSVM\":\n",
    "        loss = mAP_score_aug*self.config.alpha - mAP_score_GT\n",
    "    \n",
    "    elif self.config.optimization_framework == \"DLM\":\n",
    "        loss = (1/self.config.epsilon)*(\n",
    "            mAP_score_aug*self.config.alpha-mAP_score_std)\n",
    "        if not self.config.positive_update:\n",
    "            loss*=-1\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimization framework\")\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Add method to the class\n",
    "Model.compute_loss = compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the computation of the loss. We are left with creating the trainig schedule, this is much easier since we have kept the information on the config file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We handle the training of the model via a method called train_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def train_step(self,batch):\n",
    "    lr = self.config.lr\n",
    "    if self.config.optimizer==\"Adam\":\n",
    "        pass\n",
    "    optimizer = torch.optim.Adam(\n",
    "        self.parameters(),lr)\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         chain(self.layer1.parameters(),\n",
    "#          self.layer2.parameters(),\n",
    "#          self.layer3.parameters(),\n",
    "#          self.layer4.parameters()),lr)\n",
    "#     optimizer.zero_grad()\n",
    "    mAP_score_std,mAP_score_aug,mAP_score_GT=model.inference(batch)\n",
    "    loss= model.compute_loss(mAP_score_std,mAP_score_aug,mAP_score_GT)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    self.losses.append(loss)\n",
    "    \n",
    "Model.train_step = train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a method to determine how well we are doing in the training process. \n",
    "\n",
    "# Evaluation\n",
    "\n",
    "We want to check how accurate the model is, we do this in two steps:\n",
    "\n",
    "- First we create the scores for every element in the batch. (eval_mAP method)\n",
    "- Second: we compare how those scores compare with the real result. (apk method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mAP(batch, model):\n",
    "    imgs = batch[0]\n",
    "    labels = batch[1]\n",
    "    batch_size = len(labels)\n",
    "    \n",
    "    pos_positions, neg_positions, pos_sizes, neg_sizes = \\\n",
    "        model.get_positive_negative_splits(batch)\n",
    "    \n",
    "    batch_X_tensor = torch.from_numpy(imgs).float()\n",
    "    batch_X_tensor = Variable(batch_X_tensor,\n",
    "                                             requires_grad=False)\n",
    "\n",
    "    emb_X = model.forward(batch_X_tensor)\n",
    "    \n",
    "    query_APs =[]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "\n",
    "        current = emb_X[i]\n",
    "        \n",
    "        current_num_positive = int(pos_sizes[i])\n",
    "        current_num_negative = int(neg_sizes[i])\n",
    "        \n",
    "        # We need at least a negative sample\n",
    "        if current_num_negative==0:\n",
    "            continue        \n",
    "        \n",
    "        positive_for_current = \\\n",
    "            emb_X[pos_positions[i][:current_num_positive],:]\n",
    "            \n",
    "        negative_for_current = \\\n",
    "            emb_X[neg_positions[i][:current_num_negative],:]\n",
    "            \n",
    "        # Compute similarities\n",
    "        S_pos = torch.matmul(current,positive_for_current.t())\n",
    "        S_neg = torch.matmul(current,negative_for_current.t())\n",
    "\n",
    "        # sorts the values of the similarities\n",
    "        phi_pos, _ = S_pos.sort()\n",
    "        phi_neg, _ = S_neg.sort()\n",
    "        \n",
    "        y_true = torch.cat([torch.ones(phi_pos.shape[0]),torch.zeros(phi_neg.shape[0])])\n",
    "        \n",
    "        y_scores = torch.cat([phi_pos,phi_neg])\n",
    "        \n",
    "        AP = apk(y_true,y_scores)\n",
    "        \n",
    "        query_APs.append(AP)\n",
    "        \n",
    "    return sum(query_APs)/(1+len(query_APs))\n",
    "\n",
    "def apk(y_true,y_scores):\n",
    "    \"\"\"Computes average precision between to list of items\"\"\"\n",
    "    \n",
    "    ranks = y_scores.sort(descending=True)[1].data\n",
    "\n",
    "    actual = y_true[ranks]\n",
    "    predicted = y_scores[ranks]\n",
    "    \n",
    "    score = 0\n",
    "    num_hits = 0\n",
    "    \n",
    "    for i, p in enumerate(y_scores):\n",
    "        if y_true[i]:\n",
    "            num_hits+=1\n",
    "            score+=num_hits/(i+1)\n",
    "    num_relevant = y_true.sum()\n",
    "    \n",
    "    if num_relevant>0:\n",
    "        AP = score/num_relevant\n",
    "    else:\n",
    "        AP = None\n",
    "    \n",
    "    return AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing cell\n",
    "configDLM= ConfigDLM()\n",
    "model = Model(configDLM)\n",
    "batch=get_batch(3,4,'train')\n",
    "model.train_step(batch)\n",
    "eval_mAP(batch,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running \n",
    "\n",
    "We then instiatate our config and our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configDLM= ConfigDLM()\n",
    "model = Model(configDLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train for a number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SKIP = True\n",
    "if not SKIP:\n",
    "    EPOCHS=20\n",
    "    for i in range(EPOCHS):\n",
    "        print('Epoch %d of %d'%(i+1,EPOCHS),end='\\r')\n",
    "        batch=get_batch(128,16,'train')    \n",
    "        model.train_step(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not SKIP:\n",
    "    plt.plot(model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configDLM= ConfigDLM()\n",
    "model = Model(configDLM)\n",
    "batch=get_batch(3,4,'train')\n",
    "mAP_score_std,mAP_score_aug,mAP_score_GT=model.inference(batch)\n",
    "loss= model.compute_loss(mAP_score_std,mAP_score_aug,mAP_score_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999\r"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print(\"Epoch %d\"%i,end=\"\\r\")\n",
    "    model.train_step(batch)\n",
    "    model.losses.append(eval_mAP(get_batch(3,4,'val'),model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD0PgJ2lWOAeJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181f2c4358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(model.losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **References:**\n",
    "\n",
    "1. Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.\n",
    "\n",
    "2. Yang Song, Alexander G Schwing, Richard S Zemel, and Raquel Urtasun. Training deep neural networks via direct loss minimization. In Proceedings of The 33rd International Conference on Machine Learning, pages 21692177, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from qutip import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_units=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H_M = sum([tensor([qeye(2)  if not (i==n) else sigmax() for i in range(num_units)] )for n in range(num_units) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qeye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
